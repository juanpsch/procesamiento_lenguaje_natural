{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue5hxxkdAQJg"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Preprocesamiento con NLTK y Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kCED1hh-Ioyf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMOa4JPSCJ29"
   },
   "source": [
    "### Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gdW55pJin1rt"
   },
   "outputs": [],
   "source": [
    "simple_text = \"if she leaves now she might miss something important!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RIO7b8GjAC17"
   },
   "outputs": [],
   "source": [
    "large_text = \"Patients who in late middle age have smoked 20 cigarettes a day since their teens constitute an at-risk group. One thing they’re clearly at risk for is the acute sense of guilt that a clinician can incite, which immediately makes a consultation tense.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVHxBRNzCMOS"
   },
   "source": [
    "### 1 - Preprocesamiento con NLTK\n",
    "- Cada documento transformarlo en una lista de términos\n",
    "- Armar un vector de términos no repetidos de todos los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wM-lmmsFnC6X"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     D:\\Users\\juanp_schamun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     D:\\Users\\juanp_schamun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     D:\\Users\\juanp_schamun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     D:\\Users\\juanp_schamun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK cuenta con una extensiva documentación \n",
    "# que vale la pena consultar\n",
    "# https://www.nltk.org\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar tokenizador punkt\n",
    "nltk.download(\"punkt\")\n",
    "# Descargar la red de semántica del inglés WordNet\n",
    "# Es una extensa red semántica que puede usarse (entre otras cosas)\n",
    "# para hacer POS tagging o lematizar.\n",
    "nltk.download(\"wordnet\")\n",
    "# Descargar diccionario de stopwords del inglés\n",
    "nltk.download('stopwords')\n",
    "# Para usar NLTK 3.6.6 o superior es necesario instalar OMW 1.4 \n",
    "# (Open Multilingual WordNet)\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "soL5T-UDsZ20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if she leaves now she might miss something important!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1Er9fvFonfT1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instanciar el derivador, NLTK provee varios derivadores para elegir:\n",
    "# https://www.nltk.org/api/nltk.stem.html\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BuEob1D6nEPK"
   },
   "outputs": [],
   "source": [
    "# Instanciar el lematizador\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GE9pq3dMod6Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['if', 'she', 'leaves', 'now', 'she', 'might', 'miss', 'something', 'important', '!']\n"
     ]
    }
   ],
   "source": [
    "# Extraer los tokens de un doc\n",
    "# estos no van a ser los tokens finales, sino que serán procesados\n",
    "# aplicando lematización, filtrando, etc...\n",
    "tokens = word_tokenize(simple_text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lSdedQvVM-wN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: ['if', 'she', 'leav', 'now', 'she', 'might', 'miss', 'someth', 'import', '!']\n"
     ]
    }
   ],
   "source": [
    "# Transformar los tokens a sus respectivas palabras derivadas\n",
    "# Stemming\n",
    "nltk_stemedList = []\n",
    "for word in tokens:\n",
    "    nltk_stemedList.append(p_stemmer.stem(word))\n",
    "print(\"Stemming:\", nltk_stemedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OV3-wVBSNNaA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization: ['if', 'she', 'leaf', 'now', 'she', 'might', 'miss', 'something', 'important', '!']\n"
     ]
    }
   ],
   "source": [
    "# Transformar los tokens a sus respectivas palabras raiz\n",
    "# Lemmatization\n",
    "nltk_lemmaList = []\n",
    "for word in tokens:\n",
    "    nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
    "print(\"Lemmatization:\", nltk_lemmaList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jFzuIwPZuNqo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Miremos el conjunto de los signos de puntuación\n",
    "# que contiene la librería `string` de Python\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U47nxm8ZNiIr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation filter: ['if', 'she', 'leaf', 'now', 'she', 'might', 'miss', 'something', 'important']\n"
     ]
    }
   ],
   "source": [
    "# Quitar los signos de puntuacion\n",
    "nltk_punctuation = [w for w in nltk_lemmaList if w not in string.punctuation]\n",
    "print(\"Punctuation filter:\", nltk_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "A71y7Ecsun-u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "len(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ImlO-N45OuKG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words filter: ['leaf', 'might', 'miss', 'something', 'important']\n"
     ]
    }
   ],
   "source": [
    "# Filtrar stopwords\n",
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_sentence = [w for w in nltk_punctuation if w not in nltk_stop_words]\n",
    "print(\"Stop words filter:\", filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NrPtt2OmWBv"
   },
   "source": [
    "### 2 - Proceso completo con NLTK\n",
    "Tokenization → Lemmatization → Remove stopwords → Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3ZqTOZzDI7uv"
   },
   "outputs": [],
   "source": [
    "def nltk_process(text):\n",
    "    # Tokenization\n",
    "    nltk_tokenList = word_tokenize(text)\n",
    "      \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nltk_lemmaList = []\n",
    "    for word in nltk_tokenList:\n",
    "        nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    print(\"Lemmatization\")\n",
    "    print(nltk_lemmaList)\n",
    "\n",
    "    # Stop words\n",
    "    nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_sentence = [w for w in nltk_lemmaList if w not in nltk_stop_words]\n",
    "\n",
    "    # Filter Punctuation\n",
    "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"Remove stopword & Punctuation\")\n",
    "    print(filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CZdiop6IJpZN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization\n",
      "['Patients', 'who', 'in', 'late', 'middle', 'age', 'have', 'smoked', '20', 'cigarette', 'a', 'day', 'since', 'their', 'teen', 'constitute', 'an', 'at-risk', 'group', '.', 'One', 'thing', 'they', '’', 're', 'clearly', 'at', 'risk', 'for', 'is', 'the', 'acute', 'sense', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incite', ',', 'which', 'immediately', 'make', 'a', 'consultation', 'tense', '.']\n",
      " \n",
      "Remove stopword & Punctuation\n",
      "['Patients', 'late', 'middle', 'age', 'smoked', '20', 'cigarette', 'day', 'since', 'teen', 'constitute', 'at-risk', 'group', 'One', 'thing', '’', 'clearly', 'risk', 'acute', 'sense', 'guilt', 'clinician', 'incite', 'immediately', 'make', 'consultation', 'tense']\n",
      "Text len: 27\n"
     ]
    }
   ],
   "source": [
    "nltk_text = nltk_process(large_text)\n",
    "print(\"Text len:\", len(nltk_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M4F0ll1msUY"
   },
   "source": [
    "### 3 - Proceso completo con spaCy\n",
    "Tokenization → Lemmatization → Remove stopwords → Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "r57e9b9Omwnh"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Cargar pipeline de preprocesamiento de inglés\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_process(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenization & lemmatization\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    print(\"Tokenize+Lemmatize:\")\n",
    "    print(lemma_list)\n",
    "    \n",
    "    # Stop words\n",
    "    filtered_sentence =[]\n",
    "    for word in lemma_list:\n",
    "        # word es un string, para recuperar la información de los objetos de SpaCy\n",
    "        # necesitamos usar el string para pasar a un lexema, el objeto de SpaCy\n",
    "        # que para cada término contiene la información del preprocesamiento\n",
    "        # (se podría también directamente filtrar stopwords en el paso de lematización)\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "    \n",
    "    # Filter punctuation\n",
    "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"Remove stopword & punctuation: \")\n",
    "    print(filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9x_iKHu1pKBE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize+Lemmatize:\n",
      "['patient', 'who', 'in', 'late', 'middle', 'age', 'have', 'smoke', '20', 'cigarette', 'a', 'day', 'since', 'their', 'teen', 'constitute', 'an', 'at', '-', 'risk', 'group', '.', 'one', 'thing', 'they', '’re', 'clearly', 'at', 'risk', 'for', 'be', 'the', 'acute', 'sense', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incite', ',', 'which', 'immediately', 'make', 'a', 'consultation', 'tense', '.']\n",
      " \n",
      "Remove stopword & punctuation: \n",
      "['patient', 'late', 'middle', 'age', 'smoke', '20', 'cigarette', 'day', 'teen', 'constitute', 'risk', 'group', 'thing', 'clearly', 'risk', 'acute', 'sense', 'guilt', 'clinician', 'incite', 'immediately', 'consultation', 'tense']\n",
      "Text len: 27\n"
     ]
    }
   ],
   "source": [
    "spacy_text = spacy_process(large_text)\n",
    "print(\"Text len:\", len(nltk_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txMZ7lR-pvHB"
   },
   "source": [
    "### 4 - Conclusiones\n",
    "- NLTK no pasa a minúsculas el texto por su cuenta\n",
    "- spacy algunas palabras las reemplaza por su Tag (como \"'\")\n",
    "- spacy descompone palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Te3GgSNzpbGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|    NLTK    |    spaCy     |\n",
      "+------------+--------------+\n",
      "|  Patients  |   patient    |\n",
      "|    late    |     late     |\n",
      "|   middle   |    middle    |\n",
      "|    age     |     age      |\n",
      "|   smoked   |    smoke     |\n",
      "|     20     |      20      |\n",
      "| cigarette  |  cigarette   |\n",
      "|    day     |     day      |\n",
      "|   since    |     teen     |\n",
      "|    teen    |  constitute  |\n",
      "| constitute |     risk     |\n",
      "|  at-risk   |    group     |\n",
      "|   group    |    thing     |\n",
      "|    One     |   clearly    |\n",
      "|   thing    |     risk     |\n",
      "|     ’      |    acute     |\n",
      "|  clearly   |    sense     |\n",
      "|    risk    |    guilt     |\n",
      "|   acute    |  clinician   |\n",
      "|   sense    |    incite    |\n",
      "|   guilt    | immediately  |\n",
      "| clinician  | consultation |\n",
      "|   incite   |    tense     |\n",
      "+------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "table = PrettyTable(['NLTK', 'spaCy'])\n",
    "for nltk_word, spacy_word in zip(nltk_text, spacy_text):\n",
    "    table.add_row([nltk_word, spacy_word])\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2a - preprocesamiento.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
