{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ue5hxxkdAQJg"},"source":["<img src=\"https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Vectorización\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kCED1hh-Ioyf"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PUbfVnzIIoMj"},"outputs":[],"source":["def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"]},{"cell_type":"markdown","metadata":{"id":"DMOa4JPSCJ29"},"source":["### Datos"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RIO7b8GjAC17"},"outputs":[],"source":["corpus = np.array(['que dia es hoy', 'martes el dia de hoy es martes', 'martes muchas gracias'])"]},{"cell_type":"markdown","metadata":{"id":"8WqdaTmO8P1r"},"source":["Documento 1 --> que dia es hoy \\\n","Documento 2 --> martes el dia de hoy es martes \\\n","Documento 3 --> martes muchas gracias"]},{"cell_type":"markdown","metadata":{"id":"FVHxBRNzCMOS"},"source":["### 1 - Obtener el vocabulario del corpus (los términos utilizados)\n","- Cada documento transformarlo en una lista de términos\n","- Armar un vector de términos no repetidos de todos los documentos"]},{"cell_type":"code","execution_count":101,"metadata":{"id":"3ZqTOZzDI7uv"},"outputs":[{"data":{"text/plain":["[['que', 'dia', 'es', 'hoy'],\n"," ['martes', 'el', 'dia', 'de', 'hoy', 'es', 'martes'],\n"," ['martes', 'muchas', 'gracias']]"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["# Lista de terminos de cada documento\n","corpus_split = []\n","for docu in corpus:\n","    corpus_split.append(docu.split())    \n","corpus_split"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"data":{"text/plain":["array(['que', 'dia', 'es', 'de', 'muchas', 'hoy', 'gracias', 'martes',\n","       'el'], dtype='<U7')"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["# Vector no repetido de terminos de todos los documentos\n","terminos = []\n","for i in corpus_split:\n","    for j in i:\n","        terminos.append(j)\n","terminos = np.array(list(set(terminos)))\n","terminos"]},{"cell_type":"markdown","metadata":{"id":"RUhH983FI7It"},"source":["### 2- OneHot encoding\n","Data una lista de textos, devolver una matriz con la representación oneHotEncoding de estos"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Terminos:\n","\n","['que' 'dia' 'es' 'de' 'muchas' 'hoy' 'gracias' 'martes' 'el']\n","\n","\n","Documentos:\n","\n","['que', 'dia', 'es', 'hoy']\n","['martes', 'el', 'dia', 'de', 'hoy', 'es', 'martes']\n","['martes', 'muchas', 'gracias']\n"]}],"source":["print('Terminos:\\n')\n","print(terminos)\n","print('\\n')\n","print('Documentos:\\n')\n","for i in corpus_split:\n","    print(i)"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"data":{"text/plain":["array([[1., 1., 1., 0., 0., 1., 0., 0., 0.],\n","       [0., 1., 1., 1., 0., 1., 0., 1., 1.],\n","       [0., 0., 0., 0., 1., 0., 1., 1., 0.]])"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["ohe = np.zeros([len(corpus_split), terminos.shape[0]])\n","for i, docu in enumerate(corpus_split):\n","    for j, termino in enumerate(terminos):\n","        if termino in docu:\n","            ohe[i,j] = 1\n","ohe\n"]},{"cell_type":"markdown","metadata":{"id":"IIyWGmCpJVQL"},"source":["### 3- Vectores de frecuencia\n","Data una lista de textos, devolver una matriz con la representación de frecuencia de estos"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"yqij_7eHJbUi"},"outputs":[{"data":{"text/plain":["array([[1., 1., 1., 0., 0., 1., 0., 0., 0.],\n","       [0., 1., 1., 1., 0., 1., 0., 2., 1.],\n","       [0., 0., 0., 0., 1., 0., 1., 1., 0.]])"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["freq = np.zeros([len(corpus_split), terminos.shape[0]])\n","for i, docu in enumerate(corpus_split):\n","    for j, termino in enumerate(terminos):\n","        freq[i,j] = docu.count(termino)\n","freq"]},{"cell_type":"markdown","metadata":{"id":"z_Ot8HvWJcBu"},"source":["### 4- TF-IDF\n","Data una lista de textos, devolver una matriz con la representacion TFIDF"]},{"cell_type":"code","execution_count":120,"metadata":{"id":"waG_oWtpJjRw"},"outputs":[{"data":{"text/plain":["array([[0.47712125, 0.17609126, 0.17609126, 0.        , 0.        ,\n","        0.17609126, 0.        , 0.        , 0.        ],\n","       [0.        , 0.17609126, 0.17609126, 0.47712125, 0.        ,\n","        0.17609126, 0.        , 0.35218252, 0.47712125],\n","       [0.        , 0.        , 0.        , 0.        , 0.47712125,\n","        0.        , 0.47712125, 0.17609126, 0.        ]])"]},"execution_count":120,"metadata":{},"output_type":"execute_result"}],"source":["# Calculo el Document Frequency como la suma del OHE por cada termino\n","df = np.sum(ohe,0)\n","# Calcolu la Inverse Document Frequency\n","idf = np.log10(len(corpus_split)/df)\n","idf\n","# Calculo el (Term frequency-Inverse term frequency)\n","tf_idf = freq*idf\n","tf_idf"]},{"cell_type":"markdown","metadata":{"id":"xMcsfndWJjm_"},"source":["### 5 - Comparación de documentos\n","Realizar una funcion que reciba el corpus y el índice de un documento y devuelva los documentos ordenados por la similitud coseno"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[{"data":{"text/plain":["0.5"]},"execution_count":122,"metadata":{},"output_type":"execute_result"}],"source":["cosine_similarity(freq[0], freq[1])"]},{"cell_type":"code","execution_count":171,"metadata":{"id":"CZdiop6IJpZN"},"outputs":[],"source":["def simil(corpus, indice):\n","    \n","      # Lista de terminos de cada documento\n","      corpus_split = []\n","      for docu in corpus:\n","             corpus_split.append(docu.split())             \n","\n","      # Vector no repetido de terminos de todos los documentos\n","      terminos = []\n","      for i in corpus_split:\n","             for j in i:\n","                   terminos.append(j)\n","      terminos = np.array(list(set(terminos)))  \n","\n","      ohe = np.zeros([len(corpus_split), terminos.shape[0]])\n","      for i, docu in enumerate(corpus_split):\n","            for j, termino in enumerate(terminos):\n","                  if termino in docu:\n","                        ohe[i,j] = 1\n","\n","      cossimil = []\n","      docindex = []\n","      \n","      for j, i in enumerate(ohe):\n","            cossimil.append(cosine_similarity(ohe[indice], i))\n","            docindex.append(j)\n","            \n","      orden0 = np.stack([docindex,cossimil])\n","      orden0 = orden0[:,orden0[1,:].argsort()]\n","      orden = orden0[0][::-1].astype('int')\n","      result = corpus[orden]\n","      \n","      return result"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[{"data":{"text/plain":["array(['que dia es hoy', 'martes el dia de hoy es martes',\n","       'martes muchas gracias'], dtype='<U30')"]},"execution_count":140,"metadata":{},"output_type":"execute_result"}],"source":["corpus"]},{"cell_type":"code","execution_count":175,"metadata":{},"outputs":[{"data":{"text/plain":["array(['martes muchas gracias', 'martes el dia de hoy es martes',\n","       'que dia es hoy'], dtype='<U30')"]},"execution_count":175,"metadata":{},"output_type":"execute_result"}],"source":["simil(corpus, 2)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5fRYTpympAwJSVbric6dW","collapsed_sections":[],"name":"1a - word2vec.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
